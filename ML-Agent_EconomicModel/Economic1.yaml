behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512            # Уменьшено для ускорения обновлений
      buffer_size: 20480         # Увеличено для большего объема опыта
      learning_rate: 1.0e-4      # Уменьшено для стабильности обучения
      learning_rate_schedule: constant # Постоянная скорость обучения
      beta: 1.0e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 4               # Увеличено для лучшего использования данных
    network_settings:
      normalize: true            # Нормализация данных для ускорения обучения
      hidden_units: 256          # Увеличено количество нейронов для более сложной задачи
      num_layers: 3              # Добавлен дополнительный скрытый слой
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 1000000           # Увеличено количество шагов для более долгого обучения
    time_horizon: 128            # Увеличено для обработки более длинных эпизодов
    summary_freq: 10000          # Увеличено для снижения нагрузки на логирование
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10