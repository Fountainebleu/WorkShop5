# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Гайдукевич Евгений Олегович
- РИ230948
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.
Ход работы:
- В данном коде коэффициент корреляции может быть связан с расчетом производительности через значение tempInf.

```csharp

tempInf = ((pricesMonth[1] - pricesMonth[0]) / pricesMonth[0]) * 100;

```
- pricesMonth[0] и pricesMonth[1]: стоимость производства золота в первый и второй месяц. tempInf: процентное изменение стоимости производства между двумя месяцами. Это измеряет, насколько сильно изменяется цена между двумя месяцами.
- Этот показатель (темп изменения стоимости) используется для оценки успешности модели. Если изменение меньше или равно 6% (tempInf <= 6f), эпизод считается успешным, и агент получает положительное вознаграждение. Если изменение превышает 6%, агент получает штраф.
- Агент должен подобрать такие значения параметров (speedMove, timeMining, amountGold, pickaxeСost, profitPercentage), чтобы минимизировать tempInf. Это требует согласованности между различными действиями (например, правильное соотношение между количеством добытого золота и стоимостью кирки).
- pickaxeСost и profitPercentage напрямую влияют на стоимость добычи золота. amountGold влияет на делитель в формуле, что может уменьшить итоговую стоимость. Агенту нужно учитывать, как эти параметры влияют друг на друга, чтобы минимизировать цену.
- Установка фиксированного значения tempInf <= 6f создает барьер для обучения. Агент либо получает награду, либо штраф.


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

- Новые значения yaml-агента

```yaml

behaviors:
  Economic:
    trainer_type: ppo
    hyperparameters:
      batch_size: 512            # Уменьшено для ускорения обновлений
      buffer_size: 20480         # Увеличено для большего объема опыта
      learning_rate: 1.0e-4      # Уменьшено для стабильности обучения
      learning_rate_schedule: constant # Постоянная скорость обучения
      beta: 1.0e-2
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 4               # Увеличено для лучшего использования данных
    network_settings:
      normalize: true            # Нормализация данных для ускорения обучения
      hidden_units: 256          # Увеличено количество нейронов
      num_layers: 3              # Добавлен дополнительный скрытый слой
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    checkpoint_interval: 500000
    max_steps: 1000000           # Увеличено количество шагов для более долгого обучения
    time_horizon: 128            # Увеличено для обработки более длинных эпизодов
    summary_freq: 10000          # Увеличено для снижения нагрузки на логирование
    self_play:
      save_steps: 20000
      team_change: 100000
      swap_steps: 10000
      play_against_latest_model_ratio: 0.5
      window: 10
)

```
- batch_size - количество опытов (samples), которые обрабатываются за одну итерацию обучения.
- learning_rate - определяет, насколько сильно изменяются параметры нейросети на каждом шаге.
- learning_rate_schedule - режим изменения скорости обучения.
- buffer_size - общее количество собранных опытов перед обновлением параметров нейросети.
- num_epoch - количество проходов по данным из буфера во время одного обновления параметров.
- normalize - нормализация входных данных приводит их к одинаковому масштабу
- hidden_units - количество нейронов в каждом скрытом слое нейросети.
- num_layers - количество слоёв в нейросети между входным и выходным слоями.
- max_steps - общее количество шагов, за которые агент будет обучаться в среде.
- time_horizon - количество последовательных шагов, которые агент сохраняет для расчёта вознаграждений и обновлений политики.
- summary_freq - Частота, с которой данные о ходе обучения записываются в логи (TensorBoard).
- В итоге быстрее растёт накопленная награда, длина эпизодов не изменилась, policy loss остаётся примерно таким же и value loss стабилизирует обучение. Результаты self-play (ELO) ухудшились. Возможно, агент слишком рано прекращает исследование. Есть визуализация в графике ML-Agent-ов, который есть в репозитории.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

- Пример 1: ML-Agent находит объект на карте и идет к нему.
- Если в игре есть враги или NPC, которые должны отслеживать цели (например, преследование игрока). Пример: стражник в игре учится находить игрока, который постоянно меняет свою позицию.
- В стелс-играх (например, Assassin's Creed) агенты-охранники могут обучиться патрулированию зон для поиска игрока, который скрывается. ML-Agent позволит создать более адаптивное поведение, где охранники учатся предугадывать движения игрока на основе данных об его предыдущих действиях.
- Пример 2: Агент, перемещающийся между базой и рудником для добычи золота.
- В стратегических играх, таких как Warcraft, рабочие (агенты) должны добывать ресурсы (золото, минералы) и возвращать их на базу. ML-Agent может научиться находить оптимальные маршруты между базой и рудником, учитывать препятствия, избегать врагов и адаптироваться к расположению рудников.
- В играх с логистикой и транспортом (например, Factorio) ML-Agent может научиться строить оптимальный цикл доставки ресурсов между точками. При этом агент учитывает время, расстояние и количество переносимого ресурса.
- В итоге ML-Agent удобен для сложных, динамичных и адаптивных задач, где ручное программирование слишком сложно, неэффективно или требует учёта множества факторов. Если задача простая и предсказуемая, можно обойтись традиционным программным решением.

## Выводы

Я научился создавать ML-Agent-ов и понял как интегрировать их в юнити проект для того, чтобы решать сложные задачи при создании игры

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
